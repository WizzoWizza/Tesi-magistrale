{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"t8SPLTK-PkIN"},"outputs":[],"source":["!pip install huggingface_hub\n","!pip install langchain_community\n","!pip install sentence_transformers\n","!pip install faiss-cpu\n","!pip install bitsandbytes\n","!pip install torchmetrics --upgrade\n","!pip install bert-score\n","!pip install --upgrade datasets fsspec"]},{"cell_type":"markdown","metadata":{"id":"cn93fgrlgl81"},"source":["# Import"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7d9ATaWzEj8u"},"outputs":[],"source":["import faiss\n","import numpy as np\n","import os\n","import json\n","import re\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from sentence_transformers import CrossEncoder, util, SentenceTransformer\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n","import torch\n","import time\n","from tqdm import tqdm"]},{"cell_type":"markdown","metadata":{"id":"GBCXP5TIgiml"},"source":["# Modelli"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qcs0u2bhTavy"},"outputs":[],"source":["HF_TOKEN = \"your_api_key\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(\n","    \"meta-llama/Llama-3.1-8B-Instruct\",\n","    trust_remote_code=True,\n","    token=HF_TOKEN\n",")\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_compute_dtype=torch.float16\n",")\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    \"meta-llama/Llama-3.1-8B-Instruct\",\n","    quantization_config=bnb_config,\n","    device_map=\"auto\",\n","    trust_remote_code=True,\n","    token=HF_TOKEN\n",")"]},{"cell_type":"markdown","metadata":{"id":"OmXVizqfj1it"},"source":["# Create test set PubMedQA"]},{"cell_type":"code","source":["def extract_qa_and_joined_contexts(dataset_split):\n","    qa_pairs = []\n","    joined_contexts = []\n","\n","    for item in dataset_split:\n","        question = item[\"question\"]\n","        long_answer = item[\"long_answer\"]\n","        context_paragraphs = item[\"context\"][\"contexts\"]\n","\n","        qa_pairs.append((question, long_answer))\n","\n","        # Unisci tutti i paragrafi in un singolo blocco di contesto\n","        full_context = \"\\n\".join(context_paragraphs)\n","        joined_contexts.append(full_context)\n","\n","    return qa_pairs, joined_contexts"],"metadata":{"id":"0D_uksm4jNb3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","# Load dataset PubMedQA from HuggingFace\n","dataset = load_dataset(\"qiaojin/PubMedQA\", \"pqa_artificial\")\n","train_split = dataset[\"train\"]"],"metadata":{"id":"8g3BhDUBnK8x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get QA pairs and contexts\n","qa_pairs, contexts = extract_qa_and_joined_contexts(train_split)"],"metadata":{"id":"rZbPpJrxkoqh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v8gu9IG1On9b"},"source":["# Embeddings PubMedQA"]},{"cell_type":"markdown","metadata":{"id":"KIEZmyGe9dZ-"},"source":["# Index FAISS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NyRyubTE5_fE"},"outputs":[],"source":["# Use MXBAI-large with normalised embeddings\n","bert_model = SentenceTransformer('mixedbread-ai/mxbai-embed-large-v1')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PpR3_l8dDvQy"},"outputs":[],"source":["# 3. Calculate normalised embeddings\n","document_embeddings = bert_model.encode(\n","    contexts,\n","    normalize_embeddings=True,      # Thus normalise L2 ≈ cosine\n","    show_progress_bar=True,\n","    convert_to_numpy=True\n",")\n","\n","# 4. Create FAISS index with L2 metric\n","dimension = document_embeddings.shape[1]\n","index = faiss.IndexFlatL2(dimension)\n","\n","# 5. Add embeddings\n","index.add(document_embeddings)\n","\n","# 6. Save index\n","faiss.write_index(index, \"faiss_index_flatl2_mxbai_pubmedqa.bin\")"]},{"cell_type":"markdown","metadata":{"id":"587rX73_9wMm"},"source":["# Load embedding FAISS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QsGfRu_-wLYQ"},"outputs":[],"source":["index = faiss.read_index(\"faiss_index_flatl2_mxbai_pubmedqa.bin\")"]},{"cell_type":"markdown","metadata":{"id":"39BQ_wkA7fEh"},"source":["# Cache construction at levels L1, L2, L3\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mws2ZrjC3qvC"},"outputs":[],"source":["class HierarchicalFaissCache:\n","    THRESHOLD = 0.60  # Minimum similarity threshold (cosines)\n","\n","    def __init__(self, faiss_index, chunked_data, embedding_model, top_k=10):\n","        self.index = faiss_index\n","        self.chunked_data = chunked_data\n","        self.embedding_model = embedding_model\n","        self.top_k = top_k\n","\n","        self.L1_capacity = 25\n","        self.L2_capacity = 50\n","        self.L3_capacity = 100\n","\n","        self.L1 = []  # [(score, idx)]\n","        self.L2 = []  # [(score, idx)]\n","        self.L3 = []  # [(score, idx)]\n","\n","    def _get_query_embedding(self, query_text):\n","      embedding = self.embedding_model.encode([query_text], normalize_embeddings=True).astype('float32')\n","      return embedding\n","\n","    def _similarity(self, query_tensor, chunk_idx):\n","      chunk_embedding = self.index.reconstruct(int(chunk_idx))  # numpy array\n","      return float(np.dot(query_tensor, chunk_embedding))\n","\n","    def _retrieve_from_level(self, level, query_vector):\n","        scored_results = []\n","        seen_texts = set()  # Together to keep track of texts already seen\n","\n","        for _, idx in level:\n","            sim = self._similarity(query_vector, idx)\n","            if sim >= self.THRESHOLD:\n","\n","              current_text = self.chunked_data[idx]\n","\n","              if current_text not in seen_texts:\n","                seen_texts.add(current_text)\n","                scored_results.append((sim, idx))\n","\n","        return sorted(scored_results, key=lambda x: x[0], reverse=True)[:self.top_k]\n","\n","    def _search_faiss(self, query_tensor, query_embedding):\n","        D, I = self.index.search(query_embedding, self.top_k * 1)\n","        results = []\n","        seen_texts = set()  # Together to keep track of texts already seen\n","        for d, idx in zip(D[0], I[0]):\n","          if idx == -1:\n","              continue\n","          sim = self._similarity(query_tensor, idx)\n","\n","          # Get the text of the current chunk\n","          current_text = self.chunked_data[idx]\n","\n","          # Check if text has already been seen\n","          if current_text not in seen_texts and sim >= self.THRESHOLD:\n","              results.append((sim, idx))\n","              seen_texts.add(current_text) # Add text to the set of viewed texts\n","\n","        return sorted(results, key=lambda x: x[0], reverse=True)[:self.top_k]\n","\n","    def retrieve_from_cache(self, query_text):\n","        start_time = time.time()\n","\n","        self.demote_cache_levels()\n","        query_embedding = self._get_query_embedding(query_text)\n","        query_tensor = query_embedding[0]\n","\n","        # L1\n","        results_L1 = self._retrieve_from_level(self.L1, query_tensor)\n","        if len(results_L1) >= self.top_k:\n","            for score, idx in results_L1:\n","                self.L1 = [(s, i) for (s, i) in self.L1 if i != idx]\n","                self.L1.insert(0, (score, idx))\n","            retrieval_time = round(time.time() - start_time, 4)\n","            return [self.chunked_data[i] for _, i in results_L1], \"L1\", retrieval_time\n","\n","        # L2\n","        results_L2 = self._retrieve_from_level(self.L2, query_tensor)\n","        if results_L2:\n","            self._promote_to_L1(results_L2)\n","            retrieval_time = round(time.time() - start_time, 4)\n","            return [self.chunked_data[i] for _, i in results_L2], \"L2\", retrieval_time\n","\n","        # L3\n","        results_L3 = self._retrieve_from_level(self.L3, query_tensor)\n","        if results_L3:\n","            self._promote_to_L1(results_L3)\n","            retrieval_time = round(time.time() - start_time, 4)\n","            return [self.chunked_data[i] for _, i in results_L3], \"L3\", retrieval_time\n","\n","        # FAISS fallback\n","        new_results = self._search_faiss(query_tensor, query_embedding)\n","        self._promote_to_L1(new_results)\n","        retrieval_time = round(time.time() - start_time, 4)\n","        return [self.chunked_data[i] for _, i in new_results], \"FAISS\", retrieval_time\n","\n","    def _promote_to_L1(self, results):\n","        for score, idx in results:\n","            self.L1 = [(s, i) for (s, i) in self.L1 if i != idx]\n","            self.L1.insert(0, (score, idx))\n","        self.demote_cache_levels()\n","\n","    def demote_cache_levels(self):\n","        # L1 → L2\n","        overflow = max(0, len(self.L1) - self.L1_capacity)\n","        if overflow:\n","            demoted = self.L1[-overflow:]\n","            self.L1 = self.L1[:-overflow]\n","            demoted_idxs = {idx for _, idx in demoted}\n","            self.L2 = [(s, i) for (s, i) in self.L2 if i not in demoted_idxs]\n","            self.L2.extend(demoted)\n","            self.L2 = sorted(self.L2, key=lambda x: x[0], reverse=True)\n","\n","        # L2 → L3\n","        overflow_L2 = max(0, len(self.L2) - self.L2_capacity)\n","        if overflow_L2:\n","            demoted_L3 = self.L2[-overflow_L2:]\n","            self.L2 = self.L2[:-overflow_L2]\n","            demoted_idxs = {idx for _, idx in demoted_L3}\n","            self.L3 = [(s, i) for (s, i) in self.L3 if i not in demoted_idxs]\n","            self.L3.extend(demoted_L3)\n","            self.L3 = sorted(self.L3, key=lambda x: x[0], reverse=True)[:self.L3_capacity]"]},{"cell_type":"markdown","source":["# Cache L1"],"metadata":{"id":"qP5JBzhZTC3k"}},{"cell_type":"code","source":["class SingleLevelFaissCache:\n","    THRESHOLD = 0.50  # Minimum similarity threshold (cosines)\n","\n","    def __init__(self, faiss_index, chunked_data, embedding_model, top_k=10):\n","        self.index = faiss_index\n","        self.chunked_data = chunked_data\n","        self.embedding_model = embedding_model\n","        self.top_k = top_k\n","\n","        self.L1_capacity = 25\n","        self.L1 = []  # [(score, idx)]\n","\n","    def _get_query_embedding(self, query_text):\n","        embedding = self.embedding_model.encode([query_text], normalize_embeddings=True).astype('float32')\n","        return embedding\n","\n","    def _similarity(self, query_tensor, chunk_idx):\n","        chunk_embedding = self.index.reconstruct(int(chunk_idx))\n","        return float(np.dot(query_tensor, chunk_embedding))\n","\n","    def _retrieve_from_L1(self, query_tensor):\n","        scored_results = []\n","        seen_texts = set()\n","        for _, idx in self.L1:\n","            sim = self._similarity(query_tensor, idx)\n","            if sim >= self.THRESHOLD:\n","                text = self.chunked_data[idx]\n","                if text not in seen_texts:\n","                    seen_texts.add(text)\n","                    scored_results.append((sim, idx))\n","        return sorted(scored_results, key=lambda x: x[0], reverse=True)[:self.top_k]\n","\n","    def _search_faiss(self, query_tensor, query_embedding):\n","        D, I = self.index.search(query_embedding, self.top_k * 1)\n","        results = []\n","        seen_texts = set()\n","        for d, idx in zip(D[0], I[0]):\n","            if idx == -1:\n","                continue\n","            sim = self._similarity(query_tensor, idx)\n","            text = self.chunked_data[idx]\n","            if text not in seen_texts and sim >= self.THRESHOLD:\n","                results.append((sim, idx))\n","                seen_texts.add(text)\n","        return sorted(results, key=lambda x: x[0], reverse=True)[:self.top_k]\n","\n","    def retrieve_from_cache(self, query_text):\n","        start_time = time.time()\n","        query_embedding = self._get_query_embedding(query_text)\n","        query_tensor = query_embedding[0]\n","\n","        # Check L1 cache\n","        results_L1 = self._retrieve_from_L1(query_tensor)\n","        if len(results_L1) >= self.top_k:\n","            self._update_L1(results_L1)\n","            retrieval_time = round(time.time() - start_time, 4)\n","            return [self.chunked_data[i] for _, i in results_L1], \"L1\", retrieval_time\n","\n","        # Fallback to FAISS\n","        new_results = self._search_faiss(query_tensor, query_embedding)\n","        self._update_L1(new_results)\n","        retrieval_time = round(time.time() - start_time, 4)\n","        return [self.chunked_data[i] for _, i in new_results], \"FAISS\", retrieval_time\n","\n","    def _update_L1(self, results):\n","        for score, idx in results:\n","            self.L1 = [(s, i) for (s, i) in self.L1 if i != idx]\n","            self.L1.insert(0, (score, idx))\n","        self.L1 = self.L1[:self.L1_capacity]"],"metadata":{"id":"sQH5dYThTEhU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BYPeZBEKyAxI"},"source":["# Reranker"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u1k1gHSMyCPo"},"outputs":[],"source":["# --- Initialise reranker ---\n","reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xck7Vfw_yGHv"},"outputs":[],"source":["def rerank_with_batching(query_text, top_chunks, batch_size=8):\n","        if not top_chunks:\n","            return []\n","\n","        texts = [chunk for chunk in top_chunks]\n","        pairs = [(query_text, ctx) for ctx in texts]\n","\n","        all_scores = []\n","        # The CrossEncoder reranker already handles batching internally\n","        scores = reranker.predict(pairs, batch_size=batch_size)\n","        all_scores.extend(scores)\n","\n","        reranked = sorted(zip(all_scores, top_chunks), key=lambda x: x[0], reverse=True)\n","        return [chunk for _, chunk in reranked[:10]]"]},{"cell_type":"markdown","metadata":{"id":"ECP7cG6wmeAz"},"source":["# Experiment with HieRAG 3 levels caching"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N2s0dWsaE47j"},"outputs":[],"source":["# Set hyperparameters top_k\n","cache_manager = HierarchicalFaissCache(faiss_index=index, chunked_data=contexts, embedding_model=bert_model, top_k=20)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uiXJZzoxmfVC"},"outputs":[],"source":["output_file = \"your_output_file.ndjson\"\n","\n","questions = [qa[0] for qa in qa_pairs]\n","answer = [qa[1] for qa in qa_pairs]\n","\n","with open(output_file, \"a\", encoding=\"utf-8\") as f:\n","  for index, question in tqdm(enumerate(questions[:1000]), total=len(questions[:1000]), desc=\"Processing questions\"):\n","    t1 = time.time()\n","    query = question\n","    ground_truth = answer[index]\n","\n","    t1_retrieval = time.time()\n","    top_chunks, hit_level, retrieval_time = cache_manager.retrieve_from_cache(query)\n","\n","    # Uncomment this set of lines if you want HieRAG ++\n","    #top_chunks_rerank = rerank_with_batching(query, top_chunks, batch_size=2)\n","\n","    #search_results = \"\\n\".join(\n","    #    [chunk for chunk in top_chunks_rerank]\n","    #)\n","\n","    search_results = \"\\n\".join(\n","        [chunk for chunk in top_chunks]\n","    )\n","\n","    t2_retrieval = time.time()\n","\n","    prompt = f\"\"\"You are a biomedical assistant.\n","Analyze the context from medical literature and provide a well-reasoned, informative answer to the following research question.\n","Your answer should reflect the current scientific understanding, using the context information.\n","Context information is below.\n","\n","{search_results}\n","\n","Now answer this question:\n","\n","Question: {query}\n","\n","Answer:\"\"\"\n","\n","    t1_answer = time.time()\n","\n","    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n","\n","    output = model.generate(\n","      input_ids=inputs[\"input_ids\"],\n","      attention_mask=inputs[\"attention_mask\"],\n","      max_new_tokens=50,\n","      eos_token_id=tokenizer.eos_token_id,\n","      pad_token_id=tokenizer.eos_token_id\n","    )\n","\n","    response = tokenizer.decode(output[0], skip_special_tokens=True)\n","    t2_answer = time.time()\n","\n","    predicted_answer = (\n","          response.split(\"Answer:\")[-1].strip().split(\"\\n\")[0].strip(\" .\")\n","        )\n","\n","    t2 = time.time()\n","\n","    record = {\n","      \"prompts\": query,\n","      \"ground_truth\": ground_truth,\n","      \"responses\": predicted_answer,\n","      \"rag_time\": round(t2 - t1, 3),\n","      \"retrieval_time\": round(t2_retrieval - t1_retrieval, 3),\n","      \"answer_time\": round(t2_answer - t1_answer, 3),\n","      \"cache_hit_level\": hit_level,\n","      \"cache_L1_size\": len(cache_manager.L1)\n","      \"cache_L2_size\": len(cache_manager.L2),\n","      \"cache_L3_size\": len(cache_manager.L3)\n","    }\n","    f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")"]},{"cell_type":"markdown","metadata":{"id":"aNwMgUNPXnyN"},"source":["# Experiment HieRAG 1 level caching"]},{"cell_type":"code","source":["# Set hyperparameters top_k\n","cache_manager = SingleLevelFaissCache(faiss_index=index, chunked_data=contexts, embedding_model=bert_model, top_k=1)"],"metadata":{"id":"9OgwpaAtuxWv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Save in your output file\n","output_file = \"you_file_path.json\"\n","\n","dataset = list(dataset)\n","questions = [qa[0] for qa in dataset]\n","answer = [qa[1] for qa in dataset]\n","\n","with open(output_file, \"a\", encoding=\"utf-8\") as f:\n","  for index, question in tqdm(enumerate(questions[:500]), total=len(questions[:500]), desc=\"Processing questions\"):\n","    t1 = time.time()\n","    query = question\n","    ground_truth = answer[index]\n","\n","    top_chunks, hit_level, retrieval_time = cache_manager.retrieve_from_cache(query)\n","\n","    t1_retrieval = time.time()\n","\n","    # Uncomment this set of lines if you want HieRAG ++\n","    #top_chunks_rerank = rerank_with_batching(query, top_chunks, batch_size=2)\n","\n","    #search_results = \"\\n\".join(\n","    #    [chunk for chunk in top_chunks_rerank]\n","    #)\n","\n","    search_results = \"\\n\".join(\n","        [chunk for chunk in top_chunks]\n","    )\n","\n","    t2_retrieval = time.time()\n","\n","    prompt = f\"\"\"You are a biomedical assistant.\n","Analyze the context from medical literature and provide a well-reasoned, informative answer to the following research question.\n","Your answer should reflect the current scientific understanding, using the context information.\n","Context information is below.\n","\n","{search_results}\n","\n","Now answer this question:\n","\n","Question: {query}\n","\n","Answer:\"\"\"\n","\n","    t1_answer = time.time()\n","\n","    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n","\n","    output = model.generate(\n","      input_ids=inputs[\"input_ids\"],\n","      attention_mask=inputs[\"attention_mask\"],\n","      max_new_tokens=7,\n","      eos_token_id=tokenizer.eos_token_id,\n","      pad_token_id=tokenizer.eos_token_id\n","    )\n","\n","    response = tokenizer.decode(output[0], skip_special_tokens=True)\n","    t2_answer = time.time()\n","\n","    predicted_answer = (\n","    response.split(\"Answer:\")[-1]\n","    .split(\"(\")[0]\n","    .split(\"[Note:\")[0]\n","    .split(\"\\n\")[0]\n","    .strip(\" .\")\n","    )\n","\n","    t2 = time.time()\n","\n","    record = {\n","      \"prompts\": query,\n","      \"ground_truth\": ground_truth,\n","      \"responses\": predicted_answer,\n","      \"rag_time\": round(t2 - t1, 3),\n","      \"retrieval_time\": round(t2_retrieval - t1_retrieval, 3),\n","      \"answer_time\": round(t2_answer - t1_answer, 3),\n","      \"cache_hit_level\": hit_level,\n","      \"cache_L1_size\": len(cache_manager.L1)\n","    }\n","    f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")"],"metadata":{"id":"UFyoUIrtlxSg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# BERTScore evaluation"],"metadata":{"id":"v8MpGjfJl6Hm"}},{"cell_type":"code","source":["from bert_score import score\n","\n","# Load responses\n","path = \"your_output_file.ndjson\"\n","with open(path, \"r\", encoding=\"utf-8\") as f:\n","    data = [json.loads(line) for line in f]\n","\n","references = [entry[\"ground_truth\"] for entry in data]\n","candidates = [entry[\"responses\"] for entry in data]\n","\n","# Calculate BERTScore\n","P, R, F1 = score(candidates, references, lang=\"en\", model_type=\"bert-large-uncased\")\n","\n","# Show mean\n","print(f\"BERTScore Precision: {P.mean().item():.4f}\")\n","print(f\"BERTScore Recall:    {R.mean().item():.4f}\")\n","print(f\"BERTScore F1:        {F1.mean().item():.4f}\")"],"metadata":{"id":"-zuu9yXhl-rN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Evaluation of responses time"],"metadata":{"id":"4oTEmEQarguW"}},{"cell_type":"code","source":["def calculate_average_times(file_path):\n","    \"\"\"Calculates the average value of RAG time, Retrieval Time and Answer Time.\n","\n","    Args:\n","        file_path: Path to the JSON file containing the data.\n","\n","    Returns:\n","        A tuple containing the average values of RAG time, Retrieval Time and Answer Time.\n","    \"\"\"\n","\n","    total_rag_time = 0\n","    total_retrieval_time = 0\n","    total_answer_time = 0\n","    num_records = 0\n","\n","    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n","        for line in f:\n","            record = json.loads(line)\n","            total_rag_time += record[\"rag_time\"]\n","            total_retrieval_time += record[\"retrieval_time\"]\n","            total_answer_time += record[\"answer_time\"]\n","            num_records += 1\n","\n","    avg_rag_time = total_rag_time / num_records\n","    avg_retrieval_time = total_retrieval_time / num_records\n","    avg_answer_time = total_answer_time / num_records\n","\n","    return avg_rag_time, avg_retrieval_time, avg_answer_time\n","\n","# Percorso del file JSON\n","file_path = \"your_output_file.json\"\n","\n","# Calcola e stampa i valori medi\n","avg_rag_time, avg_retrieval_time, avg_answer_time = calculate_average_times(file_path)\n","\n","print(f\"Average RAG Time: {avg_rag_time:.3f}\")\n","print(f\"Average Retrieval Time: {avg_retrieval_time:.3f}\")\n","print(f\"Average Answer Time: {avg_answer_time:.3f}\")"],"metadata":{"id":"shx2cNlxrjb2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Rouge, Bleu e Comet"],"metadata":{"id":"ljKg5Xx-Brsf"}},{"cell_type":"code","source":["!pip install unbabel-comet rouge-score comet"],"metadata":{"id":"oqHn7ozKEHqX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","from rouge_score import rouge_scorer\n","from comet import download_model, load_from_checkpoint\n","import torch\n","\n","# Path file NDJSON\n","file_path = 'your_output_file.ndjson'\n","\n","# Loading data\n","with open(file_path, 'r') as f:\n","    data = [json.loads(line) for line in f]\n","\n","# Initialise Metrics\n","smoothie = SmoothingFunction().method4\n","rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n","\n","bleu_scores = []\n","rouge1_scores = []\n","rouge2_scores = []\n","rougeL_scores = []\n","\n","# Extract text\n","sources = []\n","predictions = []\n","references = []\n","\n","for entry in data:\n","    pred = entry['responses'].strip()\n","    ref = entry['ground_truth'].strip()\n","    predictions.append(pred)\n","    references.append(ref)\n","    sources.append(entry.get('prompts', ''))\n","\n","    # BLEU\n","    bleu = sentence_bleu([ref.split()], pred.split(), smoothing_function=smoothie)\n","    bleu_scores.append(bleu)\n","\n","    # ROUGE\n","    scores = rouge.score(ref, pred)\n","    rouge1_scores.append(scores['rouge1'].fmeasure)\n","    rouge2_scores.append(scores['rouge2'].fmeasure)\n","    rougeL_scores.append(scores['rougeL'].fmeasure)\n","\n","# Mean BLEU and ROUGE\n","print(f\"Average BLEU: {sum(bleu_scores)/len(bleu_scores):.4f}\")\n","print(f\"Average ROUGE-1: {sum(rouge1_scores)/len(rouge1_scores):.4f}\")\n","print(f\"Average ROUGE-2: {sum(rouge2_scores)/len(rouge2_scores):.4f}\")\n","print(f\"Average ROUGE-L: {sum(rougeL_scores)/len(rougeL_scores):.4f}\")\n","\n","# COMET\n","print(\"Loading COMET model...\")\n","model_path = download_model(\"Unbabel/wmt22-comet-da\")\n","comet_model = load_from_checkpoint(model_path)\n","comet_data = [{\"src\": s, \"mt\": p, \"ref\": r} for s, p, r in zip(sources, predictions, references)]\n","comet_scores = comet_model.predict(comet_data, batch_size=8, gpus=1 if torch.cuda.is_available() else 0)\n","\n","comet_scores_list = comet_scores.scores\n","\n","print(f\"Average COMET: {sum(comet_scores_list)/len(comet_scores_list):.4f}\")"],"metadata":{"id":"cPCSJg8ABziX"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"collapsed_sections":["GBCXP5TIgiml","OmXVizqfj1it","KIEZmyGe9dZ-","587rX73_9wMm","39BQ_wkA7fEh","qP5JBzhZTC3k","BYPeZBEKyAxI","ECP7cG6wmeAz","aNwMgUNPXnyN","v8MpGjfJl6Hm","4oTEmEQarguW","ljKg5Xx-Brsf"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}